<p><em><span>(The following uses a barebones example merely for the purpose of </span>illustration)</em></p>

<p>Consider that you have two species of trees in your neighbourhood. You collect their leaf size, leaf colour, height and other details. Now you plot this data on a graph using only the leaf size and the leaf colour. Something like this:</p>

<p><img src="/img/wp-content/uploads/2018/09/First.jpg" alt="" /></p>

<p>You now spot a new tree in your neighbourhood. To decide the species to which it belongs, you add it to your plot.</p>

<p><img src="/img/wp-content/uploads/2018/09/Second.jpg" alt="" /></p>

<p>How would you decide the species of this tree? Clearly, this new tree is closer to all the lower left ones. You can classify this with the others in the category.</p>

<p>But is comparing with all the lower left trees necessary? Clearly not. You can consider just two nearest trees.</p>

<p><img src="/img/wp-content/uploads/2018/09/Third.jpg" alt="" /></p>

<p>This is essentially what K-Nearest Neighbors (KNN) is. Here k = 2. KNNs can be used for both <em>classification</em> (Identifying categories) and <em>regression</em> (Predicting continuous values) tasks. For this post, I am going to focus on <strong>classification</strong>. Further, KNNs are mostly used for classification purposes.</p>

<p>Before I explain further, here is another detail. What if the new tree was located here in the graph.<figure class="wp-block-image"></figure></p>

<p><img src="/img/wp-content/uploads/2018/09/Fourth.jpg" alt="" /></p>

<p>The two nearest neighbours are conflicting as they belong to two different species. Hence, as a rule of thumb, you should take <strong>k</strong> as an odd number. And suppose the number of classes (here species) is three then you should take k = 5.</p>

<p>The distances between the points are calculated using the Euclidean distance formula. </p>

<p>Consider the following two points. The shortest distance between the points is the length of the straight line joining the two.</p>

<p>From the following figure, the distance can be calculated using the Pythagorean theorem.</p>

<script type="math/tex; mode=display">\sqrt{(x_2 - x_1)^2 + (y_1 - y_2)^2)}</script>

<p><img src="/img/wp-content/uploads/2018/12/EuclideanDist.jpg" alt="Euclidean distance plot" /></p>

<p>This equation can be extended to multi-dimensional space as follows:</p>

<script type="math/tex; mode=display">\sqrt{\sum_{i = 1}^{n}(a_i - b_i)^2}</script>

<p>K Nearest Neighbors essentially stores all the training data. The unknown class is determined based on the closet training examples. Hence, it comes in the category of <em>Lazy Learner</em>. This can be compared to <em>Eager Learners</em> (<a href="https://ashwinvaidya.com/blog/linear-regression/">Linear Regression</a>, <a href="https://ashwinvaidya.com/blog/decision-tree/">Decision Tree</a>) which builds a model to predict the class rather than remembering the training data.</p>

<h2 id="implementation-using-sklearn">Implementation using sklearn</h2>

<p>Now that we have the theory aside, let’s quickly implement it using scikit-learn. For this example, I have taken the <a href="https://archive.ics.uci.edu/ml/datasets/iris">Iris dataset</a> and stored it in <em>data.csv</em> file.</p>

<p><img src="/img/wp-content/uploads/2018/12/image-1.png" alt="" /></p>

<p><strong>Iris Dataset</strong></p>

<script src="https://gist.github.com/ashwinvaidya17/d72b02af7c767034902059b33f14868f.js"></script>

